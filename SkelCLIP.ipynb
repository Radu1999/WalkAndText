{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30789bcc-fe1a-487b-9843-a6ec148dd388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.architectures.transformer import Encoder_TRANSFORMER, Decoder_TRANSFORMER\n",
    "from src.datasets.amass import AMASS\n",
    "from src.datasets.get_dataset import get_datasets\n",
    "from pytorch_metric_learning import losses\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "import clip\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from src.utils.tensors import collate\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models.tools.losses import get_loss_function\n",
    "from src.models.rotation2xyz import Rotation2xyz\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from src.utils.action_classifier import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab679d61-3a97-4b4e-a265-733269fd4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7o5pti50) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MOTIONCLIP_DECOMP</strong> at: <a href='https://wandb.ai/docubert/text2motion/runs/7o5pti50' target=\"_blank\">https://wandb.ai/docubert/text2motion/runs/7o5pti50</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231212_191552-7o5pti50/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7o5pti50). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rchivereanu/MotionCLIP/wandb/run-20231212_192107-088wabq6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/docubert/text2motion/runs/088wabq6' target=\"_blank\">MOTIONCLIP_DECOMP</a></strong> to <a href='https://wandb.ai/docubert/text2motion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/docubert/text2motion' target=\"_blank\">https://wandb.ai/docubert/text2motion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/docubert/text2motion/runs/088wabq6' target=\"_blank\">https://wandb.ai/docubert/text2motion/runs/088wabq6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/docubert/text2motion/runs/088wabq6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f246828b9d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "wandb.login(key='93443c480bfbaa0b19be76d24f2efeb6be3319fd')\n",
    "wandb.init(project='text2motion', name='MOTIONCLIP_DECOMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd021010-e335-4b95-b255-7308b43dc7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'expname': 'exps',\n",
    "    'folder': './exps/clip',\n",
    "    'cuda': True,\n",
    "    'device': torch.device(type='cuda', index=0),\n",
    "    'batch_size': 80,\n",
    "    'num_epochs': 500,\n",
    "    'lr': 0.0002,\n",
    "    'snapshot': 20,\n",
    "    'dataset': 'babel',\n",
    "    'datapath': './data/amass/amass_db/babel_30fps_db.pt',\n",
    "    'num_frames': 60,\n",
    "    'sampling': 'conseq',\n",
    "    'sampling_step': 1,\n",
    "    'pose_rep': 'rot6d',\n",
    "    'max_len': -1,\n",
    "    'min_len': -1,\n",
    "    'num_seq_max': -1,\n",
    "    'glob': True,\n",
    "    'glob_rot': [3.141592653589793, 0, 0],\n",
    "    'translation': True,\n",
    "    'debug': False,\n",
    "    'use_action_cat_as_text_labels': True,\n",
    "    'only_60_classes': True,\n",
    "    'use_only_15_classes': False,\n",
    "    'modelname': 'motionclip_transformer_rc_rcxyz_vel',\n",
    "    'latent_dim': 512,\n",
    "    'lambda_rc': 95.0,\n",
    "    'lambda_rcxyz': 95.0,\n",
    "    'lambda_vel': 95.0,\n",
    "    'lambda_velxyz': 1.0,\n",
    "    'jointstype': 'vertices',\n",
    "    'vertstrans': False,\n",
    "    'num_layers': 8,\n",
    "    'activation': 'gelu',\n",
    "    'clip_image_losses': ['cosine'],\n",
    "    'clip_text_losses': ['cosine'],\n",
    "    'clip_lambda_mse': 1.0,\n",
    "    'clip_lambda_ce': 1.0,\n",
    "    'clip_lambda_cosine': 1.0,\n",
    "    'clip_training': '',\n",
    "    'clip_layers': 12,\n",
    "    'modeltype': 'motionclip',\n",
    "    'archiname': 'transformer',\n",
    "    'losses': ['rc', 'rcxyz', 'vel'],\n",
    "    'lambdas': {'rc': 95.0, 'rcxyz': 95.0, 'vel': 95.0},\n",
    "    'clip_lambdas': {'image': {'cosine': 1.0}, 'text': {'cosine': 1.0}},\n",
    "    'num_classes': 1,\n",
    "    'nfeats': 6,\n",
    "    'njoints': 25,\n",
    "    'outputxyz': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4491b7f5-3ce8-45fa-9204-903b13e43be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/amass/amass_db/babel_30fps_train.pt]\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "BROTHER???\n",
      "datapath used by amass is [./data/amass/amass_db/babel_30fps_vald.pt]\n",
      "BROTHER???\n",
      "BROTHER???\n"
     ]
    }
   ],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)  # Must set jit=False for trainin)\n",
    "train_dataset = get_datasets(parameters=parameters, clip_preprocess=clip_preprocess, split='train')['train']\n",
    "val_dataset = get_datasets(parameters=parameters, \n",
    "                           clip_preprocess=clip_preprocess, split='vald')['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07327ec7-0289-4d2d-874f-dea8603eb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, target):\n",
    "        # Cosine similarity between the two outputs\n",
    "        cos_sim = F.cosine_similarity(output1, output2)\n",
    "\n",
    "        # Contrastive loss calculation\n",
    "        loss_contrastive = torch.mean((1 - target) * torch.pow(1 - cos_sim, 2) +\n",
    "                                      (target) * torch.pow(torch.clamp(cos_sim - self.margin, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74c8aa16-be37-47aa-94c7-809808174dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SkelCLIP(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, outputxyz=True, train_dataset=None, val_dataset=None, pose_rep='rot6d',\n",
    "                 lambdas=None, text_cosine_lambda=1.0, latent_dim=512, glob_rot=[3.141592653589793, 0, 0], glob=True, translation=True,\n",
    "                 jointstype='vertices', vertstrans=False,\n",
    "                 image_cosine_lambda=1.0, batch_size=80, lr=1e-3, **kwargs):\n",
    "        super(SkelCLIP, self).__init__()\n",
    "        model_name = 'roberta-base'\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.text_encoder = RobertaModel.from_pretrained(model_name)\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.econder_to_latent = nn.Linear(768, latent_dim)\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "        self.pose_rep = pose_rep\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device, jit=False)\n",
    "        self.clip_model.eval()\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.outputxyz = outputxyz\n",
    "        self.clip_lambdas = {\n",
    "            'text': {'cosine': text_cosine_lambda},\n",
    "            #'image': {'cosine': text_cosine_lambda},\n",
    "        }\n",
    "        \n",
    "        self.lambdas=lambdas\n",
    "        self.glob_rot = glob_rot\n",
    "        self.glob = glob\n",
    "        self.translation = translation\n",
    "        self.ae_lambdas = lambdas\n",
    "        self.cosine_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.vertstrans = vertstrans\n",
    "        self.jointstype = jointstype\n",
    "        self.rotation2xyz = Rotation2xyz(device=torch.device(type='cpu', index=0))\n",
    "        self.param2xyz = {\"pose_rep\": self.pose_rep,\n",
    "                          \"glob_rot\": self.glob_rot,\n",
    "                          \"glob\": self.glob,\n",
    "                          \"jointstype\": self.jointstype,\n",
    "                          \"translation\": self.translation,\n",
    "                          \"vertstrans\": self.vertstrans}\n",
    "        self.contrastive = ContrastiveLoss()\n",
    "    \n",
    "    def rot2xyz(self, x, mask, get_rotations_back=False, **kwargs):\n",
    "        kargs = self.param2xyz.copy()\n",
    "        kargs.update(kwargs)\n",
    "        return self.rotation2xyz(x, mask, get_rotations_back=get_rotations_back, **kargs)    \n",
    "    \n",
    "    def forward(self, batch):\n",
    "        if self.outputxyz:\n",
    "            batch[\"x_xyz\"] = self.rot2xyz(batch[\"x\"], batch[\"mask\"])\n",
    "            \n",
    "        batch.update(self.encoder(batch))\n",
    "        batch[\"z\"] = batch[\"mu\"]\n",
    "        # decode\n",
    "        batch.update(self.decoder(batch))\n",
    "        # if we want to output xyz\n",
    "        if self.outputxyz:\n",
    "            batch[\"output_xyz\"] = self.rot2xyz(batch[\"output\"], batch[\"mask\"])\n",
    "        return batch\n",
    "    \n",
    "    def compute_ae_loss(self, batch, loop='train'):\n",
    "        mixed_loss = 0.\n",
    "        for ltype, lam in self.lambdas.items():\n",
    "            loss_function = get_loss_function(ltype) \n",
    "            loss = loss_function(batch) * lam\n",
    "            self.log(f'{ltype}_{loop}', loss, sync_dist=True, batch_size=self.batch_size)\n",
    "            mixed_loss += loss\n",
    "        return mixed_loss\n",
    "    \n",
    "    def compute_clip_loss(self, batch):\n",
    "        mixed_clip_loss = 0.\n",
    "        clip_losses = {}\n",
    "        #target = torch.eye(batch['z'].shape[0]).to(self.device)\n",
    "        for d in self.clip_lambdas.keys():\n",
    "            if len(self.clip_lambdas[d].keys()) == 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                if d == 'image':\n",
    "                    features = self.clip_model.encode_image(\n",
    "                        batch['clip_images']).float()  # preprocess is done in dataloader\n",
    "                elif d == 'text':\n",
    "                    print(batch['clip_text'])\n",
    "                    tokens = self.tokenizer(batch['clip_text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                    features = self.econder_to_latent(\n",
    "                        torch.mean(self.text_encoder(**tokens).last_hidden_state, dim=1))\n",
    "                    print(features.shape)\n",
    "                else:\n",
    "                    raise ValueError(f'Invalid clip domain [{d}]')\n",
    "\n",
    "            # normalized features\n",
    "            features_norm = features / features.norm(dim=-1, keepdim=True)\n",
    "            seq_motion_features_norm = batch[\"z\"] / batch[\"z\"].norm(dim=-1, keepdim=True)\n",
    "            # mixed_clip_loss += self.contrastive(features_norm, seq_motion_features_norm, target)\n",
    "            cos = self.cosine_sim(features_norm, seq_motion_features_norm)\n",
    "            cosine_loss = (1 - cos).mean()\n",
    "            clip_losses[f'{d}_cosine'] = cosine_loss.item()\n",
    "            mixed_clip_loss += cosine_loss * self.clip_lambdas[d]['cosine']\n",
    "            \n",
    "        return mixed_clip_loss, clip_losses\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        clip_loss, clip_losses = self.compute_clip_loss(output)\n",
    "        self.log('clip_loss_train', clip_loss, sync_dist=True, on_step=True, batch_size=self.batch_size)\n",
    "        # ae_loss = self.compute_ae_loss(output)\n",
    "        return clip_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        clip_loss, clip_losses = self.compute_clip_loss(output)\n",
    "        # ae_loss = self.compute_ae_loss(output, loop='val')\n",
    "        self.log('clip_loss_val', clip_loss, sync_dist=True, batch_size=self.batch_size)\n",
    "        return clip_loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.eval()\n",
    "        top_1_acc, top_5_acc = evaluate(self, self.val_dataset, self.val_dataloader(), {})\n",
    "        self.train()\n",
    "        self.log('top_1_acc', top_1_acc, sync_dist=True, batch_size=self.batch_size)\n",
    "        self.log('top_5_acc', top_1_acc, sync_dist=True, batch_size=self.batch_size)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                                shuffle=True, num_workers=32, collate_fn=collate)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                                shuffle=False, num_workers=32, collate_fn=collate)\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.rotation2xyz.smpl_model = self.rotation2xyz.smpl_model.to(device)\n",
    "        self = super(SkelCLIP, self).to(device)\n",
    "        return self\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8013aa5-cc2d-4013-97f8-4471b604f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20112\n",
      "BROTHER???\n",
      "{'inp': tensor([[[-4.7670e-01, -5.1837e-01, -5.8202e-01,  ..., -3.9532e-01,\n",
      "          -4.6517e-01, -5.4573e-01],\n",
      "         [ 1.0069e-01,  9.9092e-02,  7.9911e-02,  ..., -1.1173e-01,\n",
      "          -1.1524e-01, -1.1933e-01],\n",
      "         [-8.7328e-01, -8.4939e-01, -8.0924e-01,  ...,  9.1172e-01,\n",
      "           8.7769e-01,  8.2942e-01],\n",
      "         [-8.7826e-01, -8.5454e-01, -8.1267e-01,  ...,  9.1853e-01,\n",
      "           8.8518e-01,  8.3753e-01],\n",
      "         [-9.6918e-02, -9.7796e-02, -9.2281e-02,  ..., -5.3032e-02,\n",
      "          -6.9794e-02, -1.0936e-01],\n",
      "         [ 4.6825e-01,  5.1010e-01,  5.7538e-01,  ...,  3.9178e-01,\n",
      "           4.5998e-01,  5.3533e-01]],\n",
      "\n",
      "        [[ 9.6482e-01,  9.6570e-01,  9.6878e-01,  ...,  8.9487e-01,\n",
      "           9.2027e-01,  9.4676e-01],\n",
      "         [-2.5414e-01, -2.4341e-01, -2.2263e-01,  ..., -3.9268e-01,\n",
      "          -3.5102e-01, -2.8709e-01],\n",
      "         [ 6.7417e-02,  9.0409e-02,  1.0912e-01,  ...,  2.1217e-01,\n",
      "           1.7288e-01,  1.4570e-01],\n",
      "         [ 2.4121e-01,  2.2822e-01,  2.0612e-01,  ...,  3.3833e-01,\n",
      "           3.0440e-01,  2.5270e-01],\n",
      "         [ 9.5757e-01,  9.6174e-01,  9.6779e-01,  ...,  9.0683e-01,\n",
      "           9.1986e-01,  9.4307e-01],\n",
      "         [ 1.5775e-01,  1.5156e-01,  1.4455e-01,  ...,  2.5137e-01,\n",
      "           2.4737e-01,  2.1626e-01]],\n",
      "\n",
      "        [[ 9.2310e-01,  9.2900e-01,  9.3327e-01,  ...,  9.3387e-01,\n",
      "           9.3476e-01,  9.3342e-01],\n",
      "         [ 3.5699e-01,  3.4985e-01,  3.4077e-01,  ...,  3.5760e-01,\n",
      "           3.5510e-01,  3.5874e-01],\n",
      "         [-1.4300e-01, -1.2065e-01, -1.1347e-01,  ..., -2.6904e-03,\n",
      "          -1.1404e-02, -6.3620e-03],\n",
      "         [-3.1090e-01, -3.0728e-01, -3.0117e-01,  ..., -3.5182e-01,\n",
      "          -3.4902e-01, -3.5257e-01],\n",
      "         [ 9.1162e-01,  9.1093e-01,  9.1463e-01,  ...,  9.2007e-01,\n",
      "           9.2380e-01,  9.2037e-01],\n",
      "         [ 2.6886e-01,  2.7530e-01,  2.6972e-01,  ...,  1.7232e-01,\n",
      "           1.5741e-01,  1.6916e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.1373e-01,  9.1373e-01,  9.1373e-01,  ...,  9.1373e-01,\n",
      "           9.1373e-01,  9.1373e-01],\n",
      "         [ 4.0589e-01,  4.0589e-01,  4.0589e-01,  ...,  4.0589e-01,\n",
      "           4.0589e-01,  4.0589e-01],\n",
      "         [ 1.8670e-02,  1.8670e-02,  1.8670e-02,  ...,  1.8670e-02,\n",
      "           1.8670e-02,  1.8670e-02],\n",
      "         [-4.0118e-01, -4.0118e-01, -4.0118e-01,  ..., -4.0118e-01,\n",
      "          -4.0118e-01, -4.0118e-01],\n",
      "         [ 9.0850e-01,  9.0850e-01,  9.0850e-01,  ...,  9.0850e-01,\n",
      "           9.0850e-01,  9.0850e-01],\n",
      "         [-1.1701e-01, -1.1701e-01, -1.1701e-01,  ..., -1.1701e-01,\n",
      "          -1.1701e-01, -1.1701e-01]],\n",
      "\n",
      "        [[ 9.1373e-01,  9.1373e-01,  9.1373e-01,  ...,  9.1373e-01,\n",
      "           9.1373e-01,  9.1373e-01],\n",
      "         [-4.0589e-01, -4.0589e-01, -4.0589e-01,  ..., -4.0589e-01,\n",
      "          -4.0589e-01, -4.0589e-01],\n",
      "         [-1.8670e-02, -1.8670e-02, -1.8670e-02,  ..., -1.8670e-02,\n",
      "          -1.8670e-02, -1.8670e-02],\n",
      "         [ 4.0118e-01,  4.0118e-01,  4.0118e-01,  ...,  4.0118e-01,\n",
      "           4.0118e-01,  4.0118e-01],\n",
      "         [ 9.0850e-01,  9.0850e-01,  9.0850e-01,  ...,  9.0850e-01,\n",
      "           9.0850e-01,  9.0850e-01],\n",
      "         [-1.1701e-01, -1.1701e-01, -1.1701e-01,  ..., -1.1701e-01,\n",
      "          -1.1701e-01, -1.1701e-01]],\n",
      "\n",
      "        [[ 0.0000e+00, -6.7893e-04,  1.4550e-03,  ..., -4.5825e-02,\n",
      "          -4.3607e-02, -4.0255e-02],\n",
      "         [ 0.0000e+00,  1.1897e-02,  2.6303e-02,  ..., -5.4444e-01,\n",
      "          -5.4901e-01, -5.4311e-01],\n",
      "         [ 0.0000e+00, -2.6318e-03, -1.1778e-02,  ...,  1.6042e-02,\n",
      "           2.5066e-02,  4.6630e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]]), 'target': 0, 'clip_image': tensor([[[0.8209, 0.8209, 0.7771,  ..., 1.0836, 1.0398, 1.0544],\n",
      "         [0.8209, 0.8063, 0.8063,  ..., 1.0836, 1.0982, 1.0398],\n",
      "         [0.8209, 0.8501, 0.8063,  ..., 1.0690, 1.0544, 1.0252],\n",
      "         ...,\n",
      "         [1.0106, 0.9814, 0.9814,  ..., 1.1420, 1.1712, 1.1274],\n",
      "         [0.9522, 0.9960, 1.0398,  ..., 1.1566, 1.1712, 1.1566],\n",
      "         [0.9668, 1.0252, 1.0106,  ..., 1.1712, 1.1420, 1.1712]],\n",
      "\n",
      "        [[0.9493, 0.9343, 0.8893,  ..., 1.2044, 1.1594, 1.1744],\n",
      "         [0.9343, 0.9193, 0.9193,  ..., 1.2044, 1.2194, 1.1594],\n",
      "         [0.9343, 0.9643, 0.9193,  ..., 1.1894, 1.1744, 1.1444],\n",
      "         ...,\n",
      "         [1.1294, 1.0994, 1.0994,  ..., 1.2645, 1.2945, 1.2495],\n",
      "         [1.0694, 1.1144, 1.1444,  ..., 1.2795, 1.2945, 1.2795],\n",
      "         [1.0844, 1.1444, 1.1294,  ..., 1.2795, 1.2645, 1.2945]],\n",
      "\n",
      "        [[1.0794, 1.0652, 1.0225,  ..., 1.3211, 1.2785, 1.2927],\n",
      "         [1.0652, 1.0510, 1.0510,  ..., 1.3211, 1.3354, 1.2785],\n",
      "         [1.0652, 1.0936, 1.0510,  ..., 1.3069, 1.2927, 1.2643],\n",
      "         ...,\n",
      "         [1.2500, 1.2216, 1.2216,  ..., 1.3780, 1.4065, 1.3638],\n",
      "         [1.2074, 1.2358, 1.2643,  ..., 1.3922, 1.4065, 1.3922],\n",
      "         [1.2074, 1.2643, 1.2500,  ..., 1.3922, 1.3780, 1.4065]]]), 'clip_path': array(['../../data/render\\\\ACCAD_Male2MartialArtsStances_c3d_D16 - switch stance and bounce_poses_frame50.png'],\n",
      "      dtype='<U100'), 'clip_text': 'martial art', 'y': 59, 'all_categories': ['martial art', 'sports move']}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.__getitem__(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fada387c-9857-4e60-94cb-f9dd8e27fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder_TRANSFORMER(**parameters)\n",
    "decoder = Decoder_TRANSFORMER(**parameters)\n",
    "model = SkelCLIP(encoder, decoder, train_dataset=train_dataset, val_dataset=val_dataset, **parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad71a1b2-a349-4512-9d14-67b5085db851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROTHER???\n",
      "BROTHER???\n",
      "['martial art']\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m example \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_clip_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 96\u001b[0m, in \u001b[0;36mSkelCLIP.compute_clip_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# normalized features\u001b[39;00m\n\u001b[1;32m     95\u001b[0m features_norm \u001b[38;5;241m=\u001b[39m features \u001b[38;5;241m/\u001b[39m features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 96\u001b[0m seq_motion_features_norm \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# mixed_clip_loss += self.contrastive(features_norm, seq_motion_features_norm, target)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m cos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosine_sim(features_norm, seq_motion_features_norm)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'z'"
     ]
    }
   ],
   "source": [
    "example = train_dataset.__getitem__(100)\n",
    "output = model.compute_clip_loss(collate([train_dataset.__getitem__(100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da717717-64b2-4ef2-a78c-c647a1e82076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'y', 'mask', 'lengths', 'clip_images', 'clip_text', 'clip_path', 'all_categories', 'x_xyz', 'mu', 'z', 'output', 'output_xyz'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d89ff7e-f267-4d72-bae8-14e8513ab24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 6, 60])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59d81d6-ac78-4d87-95f2-b89233ca0022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5a43368-0ac7-4743-b280-891e7070c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1277c20b-04a6-48a8-b398-ce6a13508d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['mu'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f7eba2-5af9-4afb-b482-a01bb51c8347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['z'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d7834ee-b22e-4528-b032-a98e21910e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6bf345c-3b72-4692-942c-aa90d7bb8c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory ./skel_clip_exp exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder     | Encoder_TRANSFORMER | 16.9 M\n",
      "1 | decoder     | Decoder_TRANSFORMER | 25.3 M\n",
      "2 | clip_model  | CLIP                | 151 M \n",
      "3 | cosine_sim  | CosineSimilarity    | 0     \n",
      "4 | contrastive | ContrastiveLoss     | 0     \n",
      "----------------------------------------------------\n",
      "42.2 M    Trainable params\n",
      "151 M     Non-trainable params\n",
      "193 M     Total params\n",
      "773.963   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/252 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacty of 22.02 GiB of which 121.25 MiB is free. Process 7872 has 14.55 GiB memory in use. Including non-PyTorch memory, this process has 7.35 GiB memory in use. Of the allocated memory 4.74 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_float32_matmul_precision(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback], logger\u001b[38;5;241m=\u001b[39mwandb_logger, fast_dev_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, accumulate_grad_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m#,  val_check_interval=100, strategy='ddp_notebook', devices=4,\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py:180\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# when the strategy handles accumulation, we want to always call the optimizer step\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mhandles_gradient_accumulation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# -------------------\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _block_parallel_sync_behavior(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy, block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 180\u001b[0m         \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(batch_idx, closure)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 99\u001b[0m, in \u001b[0;36mSkelCLIP.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 99\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     clip_loss, clip_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_clip_loss(output)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_loss_train\u001b[39m\u001b[38;5;124m'\u001b[39m, clip_loss, sync_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 55\u001b[0m, in \u001b[0;36mSkelCLIP.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# if we want to output xyz\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputxyz:\n\u001b[0;32m---> 55\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_xyz\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrot2xyz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m, in \u001b[0;36mSkelCLIP.rot2xyz\u001b[0;34m(self, x, mask, get_rotations_back, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m kargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam2xyz\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     42\u001b[0m kargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotation2xyz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_rotations_back\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_rotations_back\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MotionCLIP/src/models/rotation2xyz.py:62\u001b[0m, in \u001b[0;36mRotation2xyz.__call__\u001b[0;34m(self, x, mask, pose_rep, translation, glob, jointstype, vertstrans, betas, beta, glob_rot, get_rotations_back, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     betas[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m beta\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# import ipdb; ipdb.set_trace()\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmpl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody_pose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_orient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_orient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# get the desirable joints\u001b[39;00m\n\u001b[1;32m     65\u001b[0m joints \u001b[38;5;241m=\u001b[39m out[jointstype]\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MotionCLIP/src/models/smpl.py:83\u001b[0m, in \u001b[0;36mSMPL.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 83\u001b[0m     smpl_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSMPL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     extra_joints \u001b[38;5;241m=\u001b[39m vertices2joints(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_regressor_extra, smpl_output\u001b[38;5;241m.\u001b[39mvertices)\n\u001b[1;32m     86\u001b[0m     all_joints \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([smpl_output\u001b[38;5;241m.\u001b[39mjoints, extra_joints], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/smplx/body_models.py:474\u001b[0m, in \u001b[0;36mSMPLLayer.forward\u001b[0;34m(self, betas, body_pose, global_orient, transl, return_verts, return_full_pose, pose2rot, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m     transl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([batch_size, \u001b[38;5;241m3\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    469\u001b[0m full_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    470\u001b[0m     [global_orient\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    471\u001b[0m      body_pose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNUM_BODY_JOINTS, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)],\n\u001b[1;32m    472\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 474\u001b[0m vertices, joints \u001b[38;5;241m=\u001b[39m \u001b[43mlbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_pose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshapedirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposedirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJ_regressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlbs_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpose2rot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m joints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvertex_joint_selector(vertices, joints)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Map the joints to the current dataset\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/motionclip/lib/python3.8/site-packages/smplx/lbs.py:238\u001b[0m, in \u001b[0;36mlbs\u001b[0;34m(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents, lbs_weights, pose2rot)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# (N x V x (J + 1)) x (N x (J + 1) x 16)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m num_joints \u001b[38;5;241m=\u001b[39m J_regressor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 238\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_joints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    241\u001b[0m homogen_coord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones([batch_size, v_posed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    242\u001b[0m                            dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    243\u001b[0m v_posed_homo \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v_posed, homogen_coord], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacty of 22.02 GiB of which 121.25 MiB is free. Process 7872 has 14.55 GiB memory in use. Including non-PyTorch memory, this process has 7.35 GiB memory in use. Of the allocated memory 4.74 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"CSNER\", name='MOTIONCLIP_DECOMP')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='top_1_acc',\n",
    "    mode='max',\n",
    "    save_top_k=2,\n",
    "    dirpath='./skel_clip_exp'\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=parameters['num_epochs'], callbacks=[checkpoint_callback], logger=wandb_logger, fast_dev_run=False, accumulate_grad_batches=100)  #,  val_check_interval=100, strategy='ddp_notebook', devices=4,\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae01628-7d31-4d4e-92d1-b51f9d52b952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c486e-fb41-4cb7-a72a-ca9304e308a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5b0b0-f796-4a82-96ff-a714b8bd03b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip",
   "language": "python",
   "name": "motionclip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
