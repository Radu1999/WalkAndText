{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40107129-0c90-4bdb-bf61-3e79eebc3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a2e871-3a90-4f82-9f73-83ef6040119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.amass import AMASS\n",
    "import clip\n",
    "from src.datasets.get_dataset import get_datasets\n",
    "import torch\n",
    "from src.datasets.tools import condense_duplicates\n",
    "import numpy as np\n",
    "from src.utils.action_label_to_idx import action_label_to_idx\n",
    "import src.utils.fixseed  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559d8e0a-47a9-4598-8306-6be515f33739",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'expname': 'exps',\n",
    "    'folder': './exps/clip',\n",
    "    'cuda': True,\n",
    "    'device': torch.device(type='cuda', index=0),\n",
    "    'batch_size': 80,\n",
    "    'num_epochs': 500,\n",
    "    'lr': 0.0002,\n",
    "    'snapshot': 20,\n",
    "    'dataset': 'babel',\n",
    "    'datapath': './data/amass/amass_db/babel_30fps_db.pt',\n",
    "    'num_frames': 60,\n",
    "    'sampling': 'conseq',\n",
    "    'sampling_step': 1,\n",
    "    'pose_rep': 'rot6d',\n",
    "    'max_len': -1,\n",
    "    'min_len': -1,\n",
    "    'num_seq_max': -1,\n",
    "    'glob': True,\n",
    "    'glob_rot': [3.141592653589793, 0, 0],\n",
    "    'translation': True,\n",
    "    'debug': False,\n",
    "    'use_action_cat_as_text_labels': False,\n",
    "    'only_60_classes': True,\n",
    "    'use_only_15_classes': False,\n",
    "    'modelname': 'motionclip_transformer_rc_rcxyz_vel',\n",
    "    'latent_dim': 512,\n",
    "    'lambda_rc': 95.0,\n",
    "    'lambda_rcxyz': 95.0,\n",
    "    'lambda_vel': 95.0,\n",
    "    'lambda_velxyz': 1.0,\n",
    "    'jointstype': 'vertices',\n",
    "    'vertstrans': False,\n",
    "    'num_layers': 8,\n",
    "    'activation': 'gelu',\n",
    "    'clip_image_losses': ['cosine'],\n",
    "    'clip_text_losses': ['cosine'],\n",
    "    'clip_lambda_mse': 1.0,\n",
    "    'clip_lambda_ce': 1.0,\n",
    "    'clip_lambda_cosine': 1.0,\n",
    "    'clip_training': '',\n",
    "    'clip_layers': 12,\n",
    "    'modeltype': 'motionclip',\n",
    "    'archiname': 'transformer',\n",
    "    'losses': ['rc', 'rcxyz', 'vel'],\n",
    "    'lambdas': {'rc': 95.0, 'rcxyz': 95.0, 'vel': 95.0},\n",
    "    'clip_lambdas': {'image': {'cosine': 1.0}, 'text': {'cosine': 1.0}},\n",
    "    'num_classes': 1,\n",
    "    'nfeats': 6,\n",
    "    'njoints': 25,\n",
    "    'outputxyz': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08cffacc-b93e-46ea-a4bf-8ef98574e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/amass/amass_db/babel_30fps_train.pt]\n",
      "datapath used by amass is [./data/amass/amass_db/babel_30fps_vald.pt]\n"
     ]
    }
   ],
   "source": [
    "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device='cpu',\n",
    "#                                     jit=False)  # Must set jit=False for training\n",
    "train_dataset = get_datasets(parameters=parameters, split='train')['train']\n",
    "val_dataset = get_datasets(parameters=parameters, split='vald')['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c64a21-66a8-4a91-ad17-63f167916b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20112\n",
      "./data/amass/amass_db/babel_30fps_train.pt\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.datapath)\n",
    "db = train_dataset.load_db()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec96cc8-0947-49c4-8aeb-bf9efac9d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vid_names', 'thetas', 'joints3d', 'clip_images', 'clip_pathes', 'text_raw_labels', 'text_proc_labels', 'action_cat'])\n",
      "['skip' 'transition' 'walk']\n"
     ]
    }
   ],
   "source": [
    "print(db.keys())\n",
    "print(condense_duplicates(db['action_cat'][100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e61934a-f5f8-419d-bbd6-aceeb7b0c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import joblib\n",
    "\n",
    "def gen_simple_dataset(db, split='train'):\n",
    "    generated_db = copy.deepcopy(db)\n",
    "    generated_db['clip_text'] = copy.deepcopy(db['text_proc_labels'])\n",
    "    curated_idx = []\n",
    "    for (idx, video_labels) in enumerate(db['text_raw_labels']):\n",
    "        condensed_labels = condense_duplicates(video_labels)\n",
    "        action = \" and \".join(condensed_labels)\n",
    "        generated_db['clip_text'][idx] = action\n",
    "    \n",
    "    print(generated_db['clip_text'][0])\n",
    "    joblib.dump(generated_db, f'./data/babel/babel_30fps_{split}.pt')\n",
    "\n",
    "#gen_simple_dataset(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8bf090-f96a-4c86-8ffe-cd2568f9d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_captions(actions, llm, sampling_params):\n",
    "    prompts = ['[INST] Describe a personâ€™s body movements who is performing the actions {} in detail [/INST]'\n",
    "               .format(action) for action in actions]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "def gen_llm_dataset(db, llm, sampling_params, split='train'):\n",
    "    generated_db = {}\n",
    "    for key in db.keys():\n",
    "        generated_db[key] = []\n",
    "    curated_idx = []\n",
    "    \n",
    "    actions = []\n",
    "    for (idx, vide_cat) in enumerate(db['action_cat']):\n",
    "        unique_cats = np.unique(vide_cat)\n",
    "        all_valid_cats = []\n",
    "        for multi_cats in unique_cats:\n",
    "            for cat in multi_cats.split(\",\"):\n",
    "                if cat not in action_label_to_idx:\n",
    "                    continue\n",
    "                \n",
    "                cat_idx = action_label_to_idx[cat]\n",
    "                if cat_idx >= 60:\n",
    "                    continue\n",
    "                all_valid_cats.extend([cat])\n",
    "\n",
    "        if len(all_valid_cats) == 0:  # No valid category available\n",
    "            continue\n",
    "        \n",
    "        for key in generated_db.keys():\n",
    "            generated_db[key].append(db[key][idx])\n",
    "        \n",
    "        choosen_cat = np.random.choice(all_valid_cats, size=1)[0]\n",
    "        # condensed_labels = condense_duplicates(video_labels)\n",
    "        # action = \" and \".join(condensed_labels)\n",
    "        actions.append(choosen_cat)\n",
    "    \n",
    "    captions = gen_captions(actions, llm, sampling_params)\n",
    "    generated_db['clip_text'] = []\n",
    "    for (idx, video_labels) in enumerate(generated_db['action_cat']):\n",
    "        generated_db['clip_text'].append(captions[idx])\n",
    "    \n",
    "    print(len(generated_db['clip_text']))\n",
    "    print(generated_db['clip_text'][0])\n",
    "    joblib.dump(generated_db, f'./data/babel_llm/babel_30fps_{split}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b1eb80-c025-4476-bf7b-b84b6ce85b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guard up' 'guard up' 'guard up' 'guard up' 'guard up' 'guard up'\n",
      " 'guard up' 'guard up' 'guard up' 'guard up' 'guard up' 'guard up'\n",
      " 'guard up' 'guard up' 'guard up' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'turn giving back'\n",
      " 'turn giving back' 'turn giving back' 'turn giving back'\n",
      " 'turn giving back' 'turn giving back' 'turn giving back'\n",
      " 'turn giving back' 'turn giving back' 'turn giving back'\n",
      " 'turn giving back' 'turn giving back' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'guard up' 'guard up'\n",
      " 'guard up' 'guard up' 'guard up' 'guard up' 'guard up' 'guard up'\n",
      " 'guard up' 'guard up' 'guard up' 'guard up' 'guard up' 'guard up'\n",
      " 'guard up' 'guard up' 'guard up' 'guard up' 'guard up' 'guard up'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'turn to original position' 'turn to original position'\n",
      " 'turn to original position' 'turn to original position'\n",
      " 'turn to original position' 'turn to original position'\n",
      " 'turn to original position' 'turn to original position'\n",
      " 'turn to original position' 'turn to original position' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'guard up' 'guard up']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset._clip_texts[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ae6243f-78c1-4945-8d1a-4cac783c9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-20 16:06:00 llm_engine.py:73] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.1', tokenizer='mistralai/Mistral-7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n",
      "INFO 12-20 16:06:13 llm_engine.py:223] # GPU blocks: 515, # CPU blocks: 2048\n",
      "INFO 12-20 16:06:14 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-20 16:06:20 model_runner.py:437] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91907c4a-9eb2-4f1b-a425-1f0386b73726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walk', 'stand', 'hand movements', 'turn', 'interact with/use object', 'arm movements', 't pose', 'step', 'backwards movement', 'raising body part', 'look', 'touch object', 'leg movements', 'forward movement', 'circular movement', 'stretch', 'jump', 'touching body part', 'sit', 'place something', 'take/pick something up', 'run', 'bend', 'throw', 'foot movements', 'a pose', 'stand up', 'lowering body part', 'sideways movement', 'move up/down incline', 'action with ball', 'kick', 'gesture', 'head movements', 'jog', 'grasp object', 'waist movements', 'lift something', 'knee movement', 'wave', 'move something', 'swing body part', 'catch', 'dance', 'lean', 'greet', 'poses', 'touching face', 'sports move', 'exercise/training', 'clean something', 'punch', 'squat', 'scratch', 'hop', 'play sport', 'stumble', 'crossing limbs', 'perform', 'martial art']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:26<00:00,  2.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./data/babel_llm/grountruth.pt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.action_label_to_idx import action_label_to_idx\n",
    "import joblib\n",
    "def gen_groundtruth():\n",
    "    \n",
    "    action_text_labels = list(action_label_to_idx.keys())\n",
    "    action_text_labels.sort(key=lambda x: action_label_to_idx[x])\n",
    "    action_text_labels = action_text_labels[:60]\n",
    "    print(action_text_labels)\n",
    "    prompts = ['[INST] Describe a personâ€™s body movements who is performing the actions {} in detail [/INST]'\n",
    "               .format(action) for action in action_text_labels]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [{'generated': output.outputs[0].text, 'orig': action_text_labels[idx]}  for (idx, output) in enumerate(outputs)]\n",
    "\n",
    "joblib.dump(gen_groundtruth(), f'./data/babel_llm/grountruth.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d7090-c0fa-4e48-8c34-eae3fa159810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a2dd4-99b2-4d09-ba7a-8910642538e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 5112/5259 [30:15<01:04,  2.30it/s] "
     ]
    }
   ],
   "source": [
    " gen_llm_dataset(train_dataset.load_db(), llm, sampling_params, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc324cf-fbb3-4287-9717-97f517a8e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_val_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel_llm_1/babel_30fps_db.pt\")\n",
    "print(len(gen_val_dataset))\n",
    "print(gen_val_dataset.__getitem__(0)['all_categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273f3c4-bf3f-4a47-acb1-17568350ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for entry in gen_val_dataset:\n",
    "    if 'all_categories'  not in entry:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716460c-5008-474a-bbc4-49a39dd22f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_val_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel/babel_30fps_db.pt\")\n",
    "print(len(simple_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4b1ed-2f3c-4834-9e01-5e8c1e535b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel_llm_1/babel_30fps_db.pt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8f79e-b091-4a47-a110-ba2c020821de",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_val = gen_val_dataset.load_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe00e31-5bc5-4736-90be-01727feea72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705b928-60da-4540-bdf3-006fb6f6a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_val_dataset._clip_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecbcc7-999a-40dd-8d7b-9da10e4cb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gen_val_dataset._clip_texts))\n",
    "print(gen_val_dataset.__getitem__(5000)['clip_text'])\n",
    "print(simple_val_dataset.__getitem__(5000)['clip_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ef2d7-9337-4c37-895c-18296566bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['[INST] Describe a personâ€™s body movements who is performing the actions {} in detail [/INST]'.format('sway')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafccc9b-6024-44c4-971f-ce15e72f3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba375840-27f3-45b5-807e-1116735e7b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip",
   "language": "python",
   "name": "motionclip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
