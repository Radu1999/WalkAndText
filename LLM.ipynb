{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40107129-0c90-4bdb-bf61-3e79eebc3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a2e871-3a90-4f82-9f73-83ef6040119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/motionclip/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.amass import AMASS\n",
    "import clip\n",
    "from src.datasets.get_dataset import get_datasets\n",
    "import torch\n",
    "from src.datasets.tools import condense_duplicates\n",
    "import numpy as np\n",
    "from src.utils.action_label_to_idx import action_label_to_idx\n",
    "import src.utils.fixseed  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559d8e0a-47a9-4598-8306-6be515f33739",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'expname': 'exps',\n",
    "    'folder': './exps/clip',\n",
    "    'cuda': True,\n",
    "    'device': torch.device(type='cuda', index=0),\n",
    "    'batch_size': 80,\n",
    "    'num_epochs': 500,\n",
    "    'lr': 0.0002,\n",
    "    'snapshot': 20,\n",
    "    'dataset': 'babel',\n",
    "    'datapath': './data/babel/babel_30fps_db.pt',\n",
    "    'num_frames': 60,\n",
    "    'sampling': 'conseq',\n",
    "    'sampling_step': 1,\n",
    "    'pose_rep': 'rot6d',\n",
    "    'max_len': -1,\n",
    "    'min_len': -1,\n",
    "    'num_seq_max': -1,\n",
    "    'glob': True,\n",
    "    'glob_rot': [3.141592653589793, 0, 0],\n",
    "    'translation': True,\n",
    "    'debug': False,\n",
    "    'use_action_cat_as_text_labels': False,\n",
    "    'only_60_classes': True,\n",
    "    'use_only_15_classes': False,\n",
    "    'modelname': 'motionclip_transformer_rc_rcxyz_vel',\n",
    "    'latent_dim': 512,\n",
    "    'lambda_rc': 95.0,\n",
    "    'lambda_rcxyz': 95.0,\n",
    "    'lambda_vel': 95.0,\n",
    "    'lambda_velxyz': 1.0,\n",
    "    'jointstype': 'vertices',\n",
    "    'vertstrans': False,\n",
    "    'num_layers': 8,\n",
    "    'activation': 'gelu',\n",
    "    'clip_image_losses': ['cosine'],\n",
    "    'clip_text_losses': ['cosine'],\n",
    "    'clip_lambda_mse': 1.0,\n",
    "    'clip_lambda_ce': 1.0,\n",
    "    'clip_lambda_cosine': 1.0,\n",
    "    'clip_training': '',\n",
    "    'clip_layers': 12,\n",
    "    'modeltype': 'motionclip',\n",
    "    'archiname': 'transformer',\n",
    "    'losses': ['rc', 'rcxyz', 'vel'],\n",
    "    'lambdas': {'rc': 95.0, 'rcxyz': 95.0, 'vel': 95.0},\n",
    "    'clip_lambdas': {'image': {'cosine': 1.0}, 'text': {'cosine': 1.0}},\n",
    "    'num_classes': 1,\n",
    "    'nfeats': 6,\n",
    "    'njoints': 25,\n",
    "    'outputxyz': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cffacc-b93e-46ea-a4bf-8ef98574e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/babel/babel_30fps_train.pt]\n",
      "datapath used by amass is [./data/babel/babel_30fps_vald.pt]\n"
     ]
    }
   ],
   "source": [
    "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device='cpu',\n",
    "#                                     jit=False)  # Must set jit=False for training\n",
    "train_dataset = get_datasets(parameters=parameters, split='train')['train']\n",
    "val_dataset = get_datasets(parameters=parameters, split='vald')['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c64a21-66a8-4a91-ad17-63f167916b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20112\n",
      "./data/babel/babel_30fps_train.pt\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.datapath)\n",
    "db = train_dataset.load_db()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec96cc8-0947-49c4-8aeb-bf9efac9d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vid_names', 'thetas', 'joints3d', 'clip_images', 'clip_pathes', 'text_raw_labels', 'text_proc_labels', 'action_cat', 'clip_text'])\n",
      "['skip' 'transition' 'walk']\n"
     ]
    }
   ],
   "source": [
    "print(db.keys())\n",
    "print(condense_duplicates(db['action_cat'][100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e61934a-f5f8-419d-bbd6-aceeb7b0c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import joblib\n",
    "\n",
    "def gen_simple_dataset(db, split='train'):\n",
    "    generated_db = copy.deepcopy(db)\n",
    "    generated_db['clip_text'] = copy.deepcopy(db['text_proc_labels'])\n",
    "    curated_idx = []\n",
    "    for (idx, video_labels) in enumerate(db['text_raw_labels']):\n",
    "        condensed_labels = condense_duplicates(video_labels)\n",
    "        action = \" and \".join(condensed_labels)\n",
    "        generated_db['clip_text'][idx] = action\n",
    "    \n",
    "    print(generated_db['clip_text'][0])\n",
    "    joblib.dump(generated_db, f'./data/babel/babel_30fps_{split}.pt')\n",
    "\n",
    "#gen_simple_dataset(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8bf090-f96a-4c86-8ffe-cd2568f9d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_captions(actions, llm, sampling_params):\n",
    "    prompts = ['[INST] Describe a personâ€™s body movements who is performing the actions {} in 30 words [/INST]'\n",
    "               .format(action) for action in actions]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "def gen_llm_dataset(db, llm, sampling_params, split='train'):\n",
    "    generated_db = {}\n",
    "    for key in db.keys():\n",
    "        generated_db[key] = []\n",
    "    curated_idx = []\n",
    "    \n",
    "    actions = []\n",
    "    for (idx, vide_cat) in enumerate(db['action_cat']):\n",
    "        unique_cats = np.unique(vide_cat)\n",
    "        all_valid_cats = []\n",
    "        for multi_cats in unique_cats:\n",
    "            for cat in multi_cats.split(\",\"):\n",
    "                if cat not in action_label_to_idx:\n",
    "                    continue\n",
    "                \n",
    "                cat_idx = action_label_to_idx[cat]\n",
    "                if cat_idx >= 60:\n",
    "                    continue\n",
    "                all_valid_cats.extend([cat])\n",
    "\n",
    "        if len(all_valid_cats) == 0:  # No valid category available\n",
    "            continue\n",
    "        \n",
    "        for key in generated_db.keys():\n",
    "            generated_db[key].append(db[key][idx])\n",
    "        \n",
    "        choosen_cat = np.random.choice(all_valid_cats, size=1)[0]\n",
    "        # condensed_labels = condense_duplicates(video_labels)\n",
    "        # action = \" and \".join(condensed_labels)\n",
    "        actions.append(choosen_cat)\n",
    "    \n",
    "    captions = gen_captions(actions, llm, sampling_params)\n",
    "    generated_db['clip_text'] = []\n",
    "    for (idx, video_labels) in enumerate(generated_db['action_cat']):\n",
    "        generated_db['clip_text'].append(captions[idx])\n",
    "        \n",
    "    print(generated_db['clip_text'][0])\n",
    "    joblib.dump(generated_db, f'./data/babel_llm_1_smaller/babel_30fps_{split}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b1eb80-c025-4476-bf7b-b84b6ce85b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guard up and transition and turn giving back and transition and guard up and transition and turn to original position and transition and guard up and transition and lower guard']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset._clip_texts[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae6243f-78c1-4945-8d1a-4cac783c9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 19:27:20,607\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-18 19:27:20,981\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-18 19:27:21 llm_engine.py:73] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.1', tokenizer='mistralai/Mistral-7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 12-18 19:27:40 llm_engine.py:222] # GPU blocks: 1375, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91907c4a-9eb2-4f1b-a425-1f0386b73726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walk', 'stand', 'hand movements', 'turn', 'interact with/use object', 'arm movements', 't pose', 'step', 'backwards movement', 'raising body part', 'look', 'touch object', 'leg movements', 'forward movement', 'circular movement', 'stretch', 'jump', 'touching body part', 'sit', 'place something', 'take/pick something up', 'run', 'bend', 'throw', 'foot movements', 'a pose', 'stand up', 'lowering body part', 'sideways movement', 'move up/down incline', 'action with ball', 'kick', 'gesture', 'head movements', 'jog', 'grasp object', 'waist movements', 'lift something', 'knee movement', 'wave', 'move something', 'swing body part', 'catch', 'dance', 'lean', 'greet', 'poses', 'touching face', 'sports move', 'exercise/training', 'clean something', 'punch', 'squat', 'scratch', 'hop', 'play sport', 'stumble', 'crossing limbs', 'perform', 'martial art']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:03<00:00, 19.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./data/babel_llm_1_smaller/grountruth.pt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.action_label_to_idx import action_label_to_idx\n",
    "import joblib\n",
    "def gen_groundtruth():\n",
    "    \n",
    "    action_text_labels = list(action_label_to_idx.keys())\n",
    "    action_text_labels.sort(key=lambda x: action_label_to_idx[x])\n",
    "    action_text_labels = action_text_labels[:60]\n",
    "    print(action_text_labels)\n",
    "    prompts = ['[INST] Describe a personâ€™s body movements who is performing the actions {} in 30 words [/INST]'\n",
    "               .format(action) for action in action_text_labels]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [{'generated': output.outputs[0].text, 'orig': action_text_labels[idx]}  for (idx, output) in enumerate(outputs)]\n",
    "\n",
    "joblib.dump(gen_groundtruth(), f'./data/babel_llm_1_smaller/grountruth.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d7090-c0fa-4e48-8c34-eae3fa159810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a2dd4-99b2-4d09-ba7a-8910642538e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " gen_llm_dataset(val_dataset.load_db(), llm, sampling_params, split='vald')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc324cf-fbb3-4287-9717-97f517a8e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/babel_llm_1/babel_30fps_vald.pt]\n",
      "7647\n",
      "['dance', 'sideways movement', 'sway']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchivereanu/MotionCLIP/src/datasets/dataset.py:198: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  step_max = (nframes - 1) // (num_frames - 1)\n"
     ]
    }
   ],
   "source": [
    "gen_val_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel_llm_1/babel_30fps_db.pt\")\n",
    "print(len(gen_val_dataset))\n",
    "print(gen_val_dataset.__getitem__(0)['all_categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5273f3c4-bf3f-4a47-acb1-17568350ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for entry in gen_val_dataset:\n",
    "    if 'all_categories'  not in entry:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c716460c-5008-474a-bbc4-49a39dd22f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/babel/babel_30fps_vald.pt]\n",
      "7647\n"
     ]
    }
   ],
   "source": [
    "simple_val_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel/babel_30fps_db.pt\")\n",
    "print(len(simple_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a4b1ed-2f3c-4834-9e01-5e8c1e535b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/babel_llm_1/babel_30fps_train.pt]\n"
     ]
    }
   ],
   "source": [
    "gen_train_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel_llm_1/babel_30fps_db.pt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda8f79e-b091-4a47-a110-ba2c020821de",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_val = gen_val_dataset.load_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe00e31-5bc5-4736-90be-01727feea72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vid_names', 'thetas', 'joints3d', 'clip_images', 'clip_pathes', 'text_raw_labels', 'text_proc_labels', 'action_cat', 'clip_text'])\n"
     ]
    }
   ],
   "source": [
    "print(db_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8705b928-60da-4540-bdf3-006fb6f6a1a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgen_val_dataset\u001b[49m\u001b[38;5;241m.\u001b[39m_clip_texts[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(gen_val_dataset._clip_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ecbcc7-999a-40dd-8d7b-9da10e4cb698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7647\n",
      " The person performing the action t-pose and transition stands with their feet shoulder-width apart and arms extended out to the sides. They move their right hand in space in front of their chest with a smooth transition, as if they are pointing or gesturing. Then, they transition to standing and move their left hand in space in front of their chest, continuing to gesture or point.\n",
      "\n",
      "Next, the person transitions back to standing and moves their right hand in space in front of their chest again, this time with a different gesture or movement. They then transition to standing and move their left hand in space in front of their chest, repeating the previous movement.\n",
      "\n",
      "The person then transitions back to standing and moves their right hand in space in front of their chest once more, this time with a different gesture or movement. They then transition to standing and move their left hand in space in front of their chest, repeating the previous movement.\n",
      "\n",
      "The person then transitions back to standing and moves both hands around their chest, this time with a different gesture or movement. They then transition to standing and extends their arms out in front of them, continuing to gesture or point.\n",
      "\n",
      "The person transitions back to standing and moves both hands around their chest once more, this time with a different gesture or movement. They then transition to standing and bends their arms at their chest, continuing to gesture or point.\n",
      "\n",
      "The person then transitions to standing and extends their arms out in front of them, this time with a different gesture or movement. They then transition to standing and transition to t-pose, this time with a different gesture or movement.\n",
      "\n",
      "The person transitions back to standing and moves their right hand in space in front of their chest, this time with a different gesture or movement. They then transition to standing and moves their left hand in space in front of their chest, repeating the previous movement.\n",
      "\n",
      "The person transitions back to standing and moves both hands around their chest, this time with a different gesture or movement. They then transition to standing and moves both hands around their chest once more, repeating the previous movement.\n",
      "\n",
      "The person transitions back to standing and moves both hands around their chest once more, this time with a different gesture or movement. They then transition to standing and waves their arms out at their side, this time with a different gesture or movement.\n",
      "\n",
      "The person then transitions back to standing and bends their arms at their chest, this time with a different gesture or movement. They then\n",
      "t-pose and transition and standing and transition and moves right hand in space in front of chest and transition and standing and transition and moves left hand in space in front of chest and transition and standing and transition and moves right hand in space in front of chest and transition and standing and transition and moves left hand in space in front of chest and transition and standing and transition and moves both hands around chest and transition and standing and transition and moves both hands around chest and transition and standing and transition and moves both hands around chest and transition and standing and transition and moves both hands around chest and transition and standing and transition and moves object from one hand to another and transition and standing and transition and moves object from one hand to another and transition and standing and bends arms at chest and transition and standing and extends arms out in front of them and transition and standing and transition and t-pose and transition and standing and transition and waves arms out at side and transition and standing and bends arms at chest and transition and standing and extends arms out in front of them and transition and standing and transition and t-pose and transition and standing and extends arms out in front of them and transition and standing and transition and moves right hand in space in front of chest and moves both hands around chest and moves right hand in space in front of chest and transition and standing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchivereanu/MotionCLIP/src/datasets/dataset.py:198: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  step_max = (nframes - 1) // (num_frames - 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(gen_val_dataset._clip_texts))\n",
    "print(gen_val_dataset.__getitem__(5000)['clip_text'])\n",
    "print(simple_val_dataset.__getitem__(5000)['clip_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a7ef2d7-9337-4c37-895c-18296566bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['[INST] Describe a personâ€™s body movements who is performing the actions {} in detail [/INST]'.format('sway')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eafccc9b-6024-44c4-971f-ce15e72f3452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When a person sways, their body movements involve a subtle back-and-forth motion, generally starting from the hips and moving downwards through the legs, knees, and ankles. The person's feet may remain planted on the ground or may lift slightly off the ground as they sway. The arms may also be involved in the swaying motion, moving back and forth in time with the body's natural rhythm. The person's upper body, including the shoulders, chest, and head, may remain relatively still or may also sway gently in rhythm with the rest of the body. Overall, the swaying motion is typically a smooth, fluid movement that is easy to detect and often accompanied by music or other forms of rhythmic stimulation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba375840-27f3-45b5-807e-1116735e7b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip",
   "language": "python",
   "name": "motionclip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
