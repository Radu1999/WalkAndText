{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40107129-0c90-4bdb-bf61-3e79eebc3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a4a921-8268-4763-ad6c-0fef061dd478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/motionclip/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from angle_emb import AnglE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f4e39ee-46a8-4b60-abd9-8cea07cf8784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "tensor([[1., 2., 3.],\n",
      "        [0., 0., 0.],\n",
      "        [4., 5., 6.],\n",
      "        [0., 1., 0.]])\n",
      "\n",
      "Filtered Matrix (non-zero rows only):\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor matrix\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                      [0.0, 0.0, 0.0],\n",
    "                      [4.0, 5.0, 6.0],\n",
    "                      [0.0, 1.0, 0.0]])\n",
    "\n",
    "# Create a mask for non-zero rows\n",
    "nonzero_rows_mask = (matrix != 0).any(dim=1)\n",
    "\n",
    "# Filter out rows where all elements are zero\n",
    "filtered_matrix = matrix[nonzero_rows_mask]\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n",
    "print(\"\\nFiltered Matrix (non-zero rows only):\")\n",
    "print(filtered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18c47325-8d57-42e5-8aae-6fc20ff0640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor(0.7973)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['mother is a relative', 'sister is a relative']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('WhereIsAI/UAE-Large-V1')\n",
    "model = AutoModel.from_pretrained('WhereIsAI/UAE-Large-V1')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings[0] @ sentence_embeddings[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac212316-8100-4249-a338-22dadffe2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9978b4d8-5de9-4881-adef-9cbad69487d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7810], device='cuda:0')\n",
      "tensor([[0.7810]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from src.utils.action_label_to_idx import action_label_to_idx\n",
    "import torch.nn.functional as F\n",
    "minim = 300\n",
    "label1 = text_encoder.encode(\"going out with the canine\", to_numpy=False)\n",
    "label1 = F.normalize(label1, p=2, dim=1)\n",
    "label2 = text_encoder.encode(\"walking the dog\", to_numpy=False)\n",
    "label2 = F.normalize(label2, p=2, dim=1)\n",
    "print(F.cosine_similarity(label1, label2))\n",
    "print(label1 @  label2.T)\n",
    "# for action in action_label_to_idx.keys():\n",
    "#     label1 = text_encoder.encode(action, to_numpy=False)\n",
    "#     label2 = text_encoder.encode(action, to_numpy=False)\n",
    "#     minim = min(minim, label1 @  label2.T)\n",
    "# print(minim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70a2e871-3a90-4f82-9f73-83ef6040119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.amass import AMASS\n",
    "import clip\n",
    "from src.datasets.get_dataset import get_datasets\n",
    "import torch\n",
    "from src.datasets.tools import condense_duplicates\n",
    "import numpy as np\n",
    "from src.utils.action_label_to_idx import action_label_to_idx\n",
    "import src.utils.fixseed  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "559d8e0a-47a9-4598-8306-6be515f33739",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'expname': 'exps',\n",
    "    'folder': './exps/clip',\n",
    "    'cuda': True,\n",
    "    'device': torch.device(type='cuda', index=0),\n",
    "    'batch_size': 80,\n",
    "    'num_epochs': 500,\n",
    "    'lr': 0.0002,\n",
    "    'snapshot': 20,\n",
    "    'dataset': 'babel',\n",
    "    'datapath': './data/babel/babel_30fps_db.pt',\n",
    "    'num_frames': 60,\n",
    "    'sampling': 'conseq',\n",
    "    'sampling_step': 1,\n",
    "    'pose_rep': 'rot6d',\n",
    "    'max_len': -1,\n",
    "    'min_len': -1,\n",
    "    'num_seq_max': -1,\n",
    "    'glob': True,\n",
    "    'glob_rot': [3.141592653589793, 0, 0],\n",
    "    'translation': True,\n",
    "    'debug': False,\n",
    "    'use_action_cat_as_text_labels': False,\n",
    "    'only_60_classes': True,\n",
    "    'use_only_15_classes': False,\n",
    "    'modelname': 'motionclip_transformer_rc_rcxyz_vel',\n",
    "    'latent_dim': 512,\n",
    "    'lambda_rc': 95.0,\n",
    "    'lambda_rcxyz': 95.0,\n",
    "    'lambda_vel': 95.0,\n",
    "    'lambda_velxyz': 1.0,\n",
    "    'jointstype': 'vertices',\n",
    "    'vertstrans': False,\n",
    "    'num_layers': 8,\n",
    "    'activation': 'gelu',\n",
    "    'clip_image_losses': ['cosine'],\n",
    "    'clip_text_losses': ['cosine'],\n",
    "    'clip_lambda_mse': 1.0,\n",
    "    'clip_lambda_ce': 1.0,\n",
    "    'clip_lambda_cosine': 1.0,\n",
    "    'clip_training': '',\n",
    "    'use_action_cat_as_text_labels': True,\n",
    "    'clip_layers': 12,\n",
    "    'modeltype': 'motionclip',\n",
    "    'archiname': 'transformer',\n",
    "    'losses': ['rc', 'rcxyz', 'vel'],\n",
    "    'lambdas': {'rc': 95.0, 'rcxyz': 95.0, 'vel': 95.0},\n",
    "    'clip_lambdas': {'image': {'cosine': 1.0}, 'text': {'cosine': 1.0}},\n",
    "    'num_classes': 1,\n",
    "    'nfeats': 6,\n",
    "    'njoints': 25,\n",
    "    'outputxyz': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08cffacc-b93e-46ea-a4bf-8ef98574e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapath used by amass is [./data/babel/babel_30fps_train.pt]\n",
      "datapath used by amass is [./data/babel/babel_30fps_vald.pt]\n"
     ]
    }
   ],
   "source": [
    "# clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device='cpu',\n",
    "#                                     jit=False)  # Must set jit=False for training\n",
    "train_dataset = get_datasets(parameters=parameters, split='train')['train']\n",
    "val_dataset = get_datasets(parameters=parameters, split='vald')['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cfa8e77c-d2e1-49b2-ae34-bfdb4e859733",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = train_dataset.load_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8258a26c-3142-4ad6-9e2c-73386de0a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vid_names', 'thetas', 'joints3d', 'clip_images', 'clip_pathes', 'text_raw_labels', 'text_proc_labels', 'action_cat', 'clip_text'])\n",
      "['lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'lie down on stomach'\n",
      " 'lie down on stomach' 'lie down on stomach' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'transition' 'transition'\n",
      " 'transition' 'transition' 'transition' 'frog pose' 'frog pose'\n",
      " 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose'\n",
      " 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose'\n",
      " 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose'\n",
      " 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose'\n",
      " 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose' 'frog pose']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_raw_labels\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_raw_labels\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlie down on stomach\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m proc_labels \u001b[38;5;241m=\u001b[39m condense_duplicates(labels)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(proc_labels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "from src.datasets.tools import condense_duplicates\n",
    "\n",
    "print(db.keys())\n",
    "print(db['text_raw_labels'][1])\n",
    "\n",
    "labels = db['text_raw_labels'][1]\n",
    "labels.append('lie down on stomach')\n",
    "proc_labels = condense_duplicates(labels)\n",
    "print(proc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c64a21-66a8-4a91-ad17-63f167916b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20112\n",
      "./data/babel/babel_30fps_train.pt\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(train_dataset.datapath)\n",
    "db = train_dataset.load_db()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcd331-7e25-499d-adaf-f2b09ce56733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec96cc8-0947-49c4-8aeb-bf9efac9d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vid_names', 'thetas', 'joints3d', 'clip_images', 'clip_pathes', 'text_raw_labels', 'text_proc_labels', 'action_cat', 'clip_text'])\n",
      "['skip' 'transition' 'walk']\n"
     ]
    }
   ],
   "source": [
    "print(db.keys())\n",
    "print(condense_duplicates(db['action_cat'][100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e61934a-f5f8-419d-bbd6-aceeb7b0c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import joblib\n",
    "\n",
    "def gen_simple_dataset(db, split='train'):\n",
    "    generated_db = copy.deepcopy(db)\n",
    "    generated_db['clip_text'] = copy.deepcopy(db['text_proc_labels'])\n",
    "    curated_idx = []\n",
    "    for (idx, video_labels) in enumerate(db['text_raw_labels']):\n",
    "        condensed_labels = condense_duplicates(video_labels)\n",
    "        action = \" and \".join(condensed_labels)\n",
    "        generated_db['clip_text'][idx] = action\n",
    "    \n",
    "    print(generated_db['clip_text'][0])\n",
    "    joblib.dump(generated_db, f'./data/babel/babel_30fps_{split}.pt')\n",
    "\n",
    "#gen_simple_dataset(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8bf090-f96a-4c86-8ffe-cd2568f9d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_captions(llm, sampling_params):\n",
    "    actions = list(action_label_to_idx.keys())[:60]\n",
    "    prompts = ['[INST] Describe a person’s body movements who is performing the actions {} in detail [/INST]'\n",
    "               .format(action) for action in actions]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    initial_results = [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "    captions_mapping = {}\n",
    "    for (idx, action) in enumerate(actions):\n",
    "        captions_mapping[action] = [initial_results[idx]]\n",
    "    prompts = []\n",
    "    for result in initial_results:\n",
    "        prompts.extend(['[INST] Paraphrase the following description: {} [/INST]'.format(result)] * 1000)\n",
    "        \n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for (idx, output) in enumerate(outputs):\n",
    "        captions_mapping[actions[idx // 1000]].append(output.outputs[0].text)\n",
    "        \n",
    "    return captions_mapping\n",
    "    \n",
    "def gen_llm_dataset(db, llm, sampling_params, split='train'):\n",
    "    generated_db = {}\n",
    "    for key in db.keys():\n",
    "        generated_db[key] = []\n",
    "    curated_idx = []\n",
    "    \n",
    "    \n",
    "    for (idx, vide_cat) in enumerate(db['action_cat'][:1]):\n",
    "        unique_cats = np.unique(vide_cat)\n",
    "        all_valid_cats = []\n",
    "        for multi_cats in unique_cats:\n",
    "            for cat in multi_cats.split(\",\"):\n",
    "                if cat not in action_label_to_idx:\n",
    "                    continue\n",
    "                \n",
    "                cat_idx = action_label_to_idx[cat]\n",
    "                if cat_idx >= 60:\n",
    "                    continue\n",
    "                all_valid_cats.extend([cat])\n",
    "\n",
    "        if len(all_valid_cats) == 0:  # No valid category available\n",
    "            continue\n",
    "        \n",
    "        for key in generated_db.keys():\n",
    "            generated_db[key].append(db[key][idx])\n",
    "        \n",
    "        choosen_cat = np.random.choice(all_valid_cats, size=1)[0]\n",
    "        # condensed_labels = condense_duplicates(video_labels)\n",
    "        # action = \" and \".join(condensed_labels)\n",
    "        actions.append(choosen_cat)\n",
    "    print(actions)\n",
    "    captions = gen_captions(actions, llm, sampling_params)\n",
    "    generated_db['clip_text'] = []\n",
    "    for (idx, video_labels) in enumerate(generated_db['action_cat']):\n",
    "        generated_db['clip_text'].append(captions[idx])\n",
    "    \n",
    "    print(len(generated_db['clip_text']))\n",
    "    print(generated_db['clip_text'][0])\n",
    "    joblib.dump(generated_db, f'./data/babel_llm_multiple/babel_30fps_{split}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b1eb80-c025-4476-bf7b-b84b6ce85b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guard up and transition and turn giving back and transition and guard up and transition and turn to original position and transition and guard up and transition and lower guard']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset._clip_texts[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ae6243f-78c1-4945-8d1a-4cac783c9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 18:25:22,774\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-22 18:25:23,153\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-22 18:25:23 llm_engine.py:73] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.1', tokenizer='mistralai/Mistral-7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 12-22 18:25:37 llm_engine.py:222] # GPU blocks: 1375, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f276bc-d814-4d32-b8c0-eae236cc1e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 60/60 [00:19<00:00,  3.09it/s]\n",
      "Processed prompts:  44%|████▎     | 26108/60000 [2:23:34<8:04:54,  1.16it/s] "
     ]
    }
   ],
   "source": [
    "captions = gen_captions(llm, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd5d8d-6f9e-4a81-84e6-ff0188e0ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(captions['touch object'][:5])\n",
    "joblib.dump(captions, f'./data/multiple_captions.pt')\n",
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91907c4a-9eb2-4f1b-a425-1f0386b73726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.action_label_to_idx import action_label_to_idx\n",
    "import joblib\n",
    "def gen_groundtruth():\n",
    "    \n",
    "    action_text_labels = list(action_label_to_idx.keys())\n",
    "    action_text_labels.sort(key=lambda x: action_label_to_idx[x])\n",
    "    action_text_labels = action_text_labels[:60]\n",
    "    print(action_text_labels)\n",
    "    prompts = ['[INST] Describe a person’s body movements who is performing the actions {} in detail [/INST]'\n",
    "               .format(action) for action in action_text_labels]\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [{'generated': output.outputs[0].text, 'orig': action_text_labels[idx]}  for (idx, output) in enumerate(outputs)]\n",
    "\n",
    "#joblib.dump(gen_groundtruth(), f'./data/babel_llm/grountruth.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d7090-c0fa-4e48-8c34-eae3fa159810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a2dd4-99b2-4d09-ba7a-8910642538e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " gen_llm_dataset(val_dataset.load_db(), llm, sampling_params, split='vald')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc324cf-fbb3-4287-9717-97f517a8e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_val_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel_llm_1/babel_30fps_db.pt\")\n",
    "print(len(gen_val_dataset))\n",
    "print(gen_val_dataset.__getitem__(0)['all_categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273f3c4-bf3f-4a47-acb1-17568350ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for entry in gen_val_dataset:\n",
    "    if 'all_categories'  not in entry:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716460c-5008-474a-bbc4-49a39dd22f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_val_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel/babel_30fps_db.pt\")\n",
    "print(len(simple_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4b1ed-2f3c-4834-9e01-5e8c1e535b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train_dataset = AMASS(clip_preprocess=clip_preprocess, datapath=\"./data/babel_llm_1/babel_30fps_db.pt\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8f79e-b091-4a47-a110-ba2c020821de",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_val = gen_val_dataset.load_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe00e31-5bc5-4736-90be-01727feea72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705b928-60da-4540-bdf3-006fb6f6a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_val_dataset._clip_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecbcc7-999a-40dd-8d7b-9da10e4cb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gen_val_dataset._clip_texts))\n",
    "print(gen_val_dataset.__getitem__(5000)['clip_text'])\n",
    "print(simple_val_dataset.__getitem__(5000)['clip_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ef2d7-9337-4c37-895c-18296566bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['[INST] Describe a person’s body movements who is performing the actions {} in detail [/INST]'.format('sway')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafccc9b-6024-44c4-971f-ce15e72f3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba375840-27f3-45b5-807e-1116735e7b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip",
   "language": "python",
   "name": "motionclip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
