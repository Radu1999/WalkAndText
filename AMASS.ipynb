{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6400d871-ff6d-485a-ada6-bf3f2743e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from src.datasets.dataset import Dataset\n",
    "from src.config import ROT_CONVENTION_TO_ROT_NUMBER\n",
    "from src import config\n",
    "from PIL import Image\n",
    "import sys\n",
    "import clip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ff046d-9c7b-4d3b-9fb6-e94d81557c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rchivereanu/MotionCLIP\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0c8dfe-474c-4a0e-b797-4c5d7de5944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('')\n",
    "\n",
    "action2motion_joints = [8, 1, 2, 3, 4, 5, 6, 7, 0, 9, 10, 11, 12, 13, 14, 21, 24, 38]  # [18,]\n",
    "\n",
    "from src.utils.action_label_to_idx import action_label_to_idx, idx_to_action_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda9b59f-c759-4dec-bca3-f6c364358737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMASS(Dataset):\n",
    "    dataname = \"amass\"\n",
    "\n",
    "    def __init__(self, datapath=\"data/amass/amass_db/babel_30fps_db.pt\", split=\"vald\", use_z=1, **kwargs):\n",
    "        assert '_db.pt' in datapath\n",
    "        self.datapath = datapath.replace('_db.pt', '_{}.pt'.format(split))\n",
    "        assert os.path.exists(self.datapath)\n",
    "        print('datapath used by amass is [{}]'.format(self.datapath))\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dataname = \"amass\"\n",
    "\n",
    "        # FIXME - hardcoded:\n",
    "        self.rot_convention = 'legacy'\n",
    "        self.use_betas = False\n",
    "        self.use_gender = False\n",
    "        self.use_body_features = False\n",
    "        if 'clip_preprocess' in kwargs.keys():\n",
    "            self.clip_preprocess = kwargs['clip_preprocess']\n",
    "\n",
    "        self.use_z = (use_z != 0)\n",
    "\n",
    "        # keep_actions = [6, 7, 8, 9, 22, 23, 24, 38, 80, 93, 99, 100, 102]\n",
    "        dummy_class = [0]\n",
    "        genders = config.GENDERS\n",
    "        self.num_classes = len(dummy_class)\n",
    "\n",
    "        self.db = self.load_db()\n",
    "        self._joints3d = []\n",
    "        self._poses = []\n",
    "        self._num_frames_in_video = []\n",
    "        self._actions = []\n",
    "        self._betas = []\n",
    "        self._genders = []\n",
    "        self._heights = []\n",
    "        self._masses = []\n",
    "        self._clip_images = []\n",
    "        self._clip_texts = []\n",
    "        self._clip_pathes = []\n",
    "        self._actions_cat = []\n",
    "        self.clip_label_text = \"text_raw_labels\"  # \"text_proc_labels\"\n",
    "\n",
    "        seq_len = 100\n",
    "        n_sequences = len(self.db['thetas'])\n",
    "        # split sequences\n",
    "        for seq_idx in range(n_sequences):\n",
    "            n_sub_seq = self.db['thetas'][seq_idx].shape[0] // seq_len\n",
    "            if n_sub_seq == 0: continue\n",
    "            n_frames_in_use = n_sub_seq * seq_len\n",
    "            joints3d = np.split(self.db['joints3d'][seq_idx][:n_frames_in_use], n_sub_seq)\n",
    "            poses = np.split(self.db['thetas'][seq_idx][:n_frames_in_use], n_sub_seq)\n",
    "            self._joints3d.extend(joints3d)\n",
    "            self._poses.extend(poses)\n",
    "            self._num_frames_in_video.extend([seq_len] * n_sub_seq)\n",
    "\n",
    "            if 'action_cat' in self.db:\n",
    "                self._actions_cat.extend(np.split(self.db['action_cat'][seq_idx][:n_frames_in_use], n_sub_seq))\n",
    "\n",
    "            if self.use_betas:\n",
    "                self._betas.extend(np.split(self.db['betas'][seq_idx][:n_frames_in_use], n_sub_seq))\n",
    "            if self.use_gender:\n",
    "                self._genders.extend([str(self.db['genders'][seq_idx]).replace(\"b'female'\", \"female\").replace(\"b'male'\",\n",
    "                                                                                                              \"male\")] * n_sub_seq)\n",
    "            if self.use_body_features:\n",
    "                self._heights.extend([self.db['heights'][seq_idx]] * n_sub_seq)\n",
    "                self._masses.extend([self.db['masses'][seq_idx]] * n_sub_seq)\n",
    "            if 'clip_images' in self.db.keys():\n",
    "                images = [np.squeeze(e) for e in np.split(self.db['clip_images'][seq_idx][:n_sub_seq], n_sub_seq)]\n",
    "                processed_images = [self.clip_preprocess(Image.fromarray(img)) for img in images]\n",
    "                self._clip_images.extend(processed_images)\n",
    "            if self.clip_label_text in self.db:\n",
    "                self._clip_texts.extend(np.split(self.db[self.clip_label_text][seq_idx][:n_frames_in_use], n_sub_seq))\n",
    "            if 'clip_pathes' in self.db:\n",
    "                self._clip_pathes.extend(np.split(self.db['clip_pathes'][seq_idx][:n_sub_seq], n_sub_seq))\n",
    "            if 'clip_images_emb' in self.db.keys():\n",
    "                self._clip_images_emb.extend(np.split(self.db['clip_images_emb'][seq_idx][:n_sub_seq], n_sub_seq))\n",
    "\n",
    "\n",
    "\n",
    "            actions = [0] * n_sub_seq\n",
    "            self._actions.extend(actions)\n",
    "\n",
    "        assert len(self._num_frames_in_video) == len(self._poses) == len(self._joints3d) == len(self._actions)\n",
    "        if self.use_betas:\n",
    "            assert len(self._poses) == len(self._betas)\n",
    "        if self.use_gender:\n",
    "            assert len(self._poses) == len(self._genders)\n",
    "        if 'clip_images' in self.db.keys():\n",
    "            assert len(self._poses) == len(self._clip_images)\n",
    "\n",
    "        self._actions = np.array(self._actions)\n",
    "        self._num_frames_in_video = np.array(self._num_frames_in_video)\n",
    "\n",
    "        N = len(self._poses)\n",
    "        # same set for training and testing\n",
    "        self._train = np.arange(N)\n",
    "        self._test = np.arange(N)\n",
    "\n",
    "        self._action_to_label = {x: i for i, x in enumerate(dummy_class)}\n",
    "        self._label_to_action = {i: x for i, x in enumerate(dummy_class)}\n",
    "\n",
    "        self._gender_to_label = {x: i for i, x in enumerate(genders)}\n",
    "        self._label_to_gender = {i: x for i, x in enumerate(genders)}\n",
    "\n",
    "        self._action_classes = idx_to_action_label\n",
    "\n",
    "    def load_db(self):\n",
    "        # Load amass dataset encoded to a .db file\n",
    "        # The loaded data is structured:\n",
    "        # {\n",
    "        #     'theta': [data_size, 82] (float64) (structured [pose(72), betas(10)])\n",
    "        #     'vid_name': [data_size] (str)\n",
    "        # }\n",
    "        # data_size should be [16275369]\n",
    "        db_file = self.datapath\n",
    "        db = joblib.load(db_file)\n",
    "        # print(db_file)\n",
    "        # print(db['clip_images'])\n",
    "        if 'clip_images' in db and len(db['clip_images']) and db['clip_images'][0] is None:  # No images added\n",
    "            del db['clip_images']\n",
    "\n",
    "        return db\n",
    "\n",
    "    def _load_joints3D(self, ind, frame_ix):\n",
    "        joints3D = self._joints3d[ind][frame_ix]\n",
    "        return joints3D\n",
    "\n",
    "    def _load_rotvec(self, ind, frame_ix):\n",
    "        pose = self._poses[ind][frame_ix, :].reshape(-1, ROT_CONVENTION_TO_ROT_NUMBER[self.rot_convention] + 1,\n",
    "                                                     3)  # +1 for global orientation\n",
    "        return pose\n",
    "\n",
    "    def _load_betas(self, ind, frame_ix):\n",
    "        betas = self._betas[ind][frame_ix].transpose((1, 0))\n",
    "        return betas\n",
    "\n",
    "    def _load_gender(self, ind, frame_ix):\n",
    "        gender = self._gender_to_label[self._genders[ind]]\n",
    "        return gender\n",
    "\n",
    "    def _load_body_features(self, ind, frame_ix):\n",
    "        return {'mass': float(self._masses[ind]), 'height': float(self._heights[ind])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedae25-854a-4922-ac72-acac56781944",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device,\n",
    "                                    jit=False)  # Must set jit=False for training\n",
    "dataset = AMASS(clip_preprocess=clip_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea01947-108e-4276-ba0e-441c71ff50f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inp': tensor([[[-9.9903e-01],\n",
      "         [ 1.8858e-04],\n",
      "         [-4.3969e-02],\n",
      "         [-4.3962e-02],\n",
      "         [ 1.3173e-02],\n",
      "         [ 9.9895e-01]],\n",
      "\n",
      "        [[ 9.9699e-01],\n",
      "         [-5.9894e-02],\n",
      "         [ 4.9313e-02],\n",
      "         [ 6.2373e-02],\n",
      "         [ 9.9678e-01],\n",
      "         [-5.0372e-02]],\n",
      "\n",
      "        [[ 9.9315e-01],\n",
      "         [ 8.9507e-02],\n",
      "         [-7.5099e-02],\n",
      "         [-9.1164e-02],\n",
      "         [ 9.9566e-01],\n",
      "         [-1.8931e-02]],\n",
      "\n",
      "        [[ 9.9991e-01],\n",
      "         [ 1.5723e-03],\n",
      "         [ 1.3193e-02],\n",
      "         [-1.4346e-03],\n",
      "         [ 9.9994e-01],\n",
      "         [-1.0437e-02]],\n",
      "\n",
      "        [[ 9.9431e-01],\n",
      "         [ 8.9540e-02],\n",
      "         [-5.7710e-02],\n",
      "         [-8.3588e-02],\n",
      "         [ 9.9163e-01],\n",
      "         [ 9.8400e-02]],\n",
      "\n",
      "        [[ 9.9546e-01],\n",
      "         [-7.9831e-02],\n",
      "         [-5.1855e-02],\n",
      "         [ 8.1345e-02],\n",
      "         [ 9.9630e-01],\n",
      "         [ 2.7762e-02]],\n",
      "\n",
      "        [[ 9.9702e-01],\n",
      "         [-3.5771e-02],\n",
      "         [-6.8321e-02],\n",
      "         [ 2.8506e-02],\n",
      "         [ 9.9412e-01],\n",
      "         [-1.0451e-01]],\n",
      "\n",
      "        [[ 9.8481e-01],\n",
      "         [-7.4446e-02],\n",
      "         [ 1.5686e-01],\n",
      "         [ 5.7004e-02],\n",
      "         [ 9.9197e-01],\n",
      "         [ 1.1290e-01]],\n",
      "\n",
      "        [[ 9.8929e-01],\n",
      "         [-2.5195e-02],\n",
      "         [-1.4377e-01],\n",
      "         [ 4.1634e-02],\n",
      "         [ 9.9278e-01],\n",
      "         [ 1.1251e-01]],\n",
      "\n",
      "        [[ 9.9968e-01],\n",
      "         [-2.0961e-02],\n",
      "         [-1.4387e-02],\n",
      "         [ 2.1916e-02],\n",
      "         [ 9.9733e-01],\n",
      "         [ 6.9725e-02]],\n",
      "\n",
      "        [[ 1.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 1.0000e+00],\n",
      "         [ 0.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 1.0000e+00],\n",
      "         [ 0.0000e+00]],\n",
      "\n",
      "        [[ 9.9944e-01],\n",
      "         [ 8.7779e-03],\n",
      "         [ 3.2220e-02],\n",
      "         [-8.9696e-03],\n",
      "         [ 9.9994e-01],\n",
      "         [ 5.8074e-03]],\n",
      "\n",
      "        [[ 8.6991e-01],\n",
      "         [ 4.8550e-01],\n",
      "         [-8.6897e-02],\n",
      "         [-4.8948e-01],\n",
      "         [ 8.7146e-01],\n",
      "         [-3.1171e-02]],\n",
      "\n",
      "        [[ 8.6503e-01],\n",
      "         [-4.8693e-01],\n",
      "         [ 1.2089e-01],\n",
      "         [ 4.9508e-01],\n",
      "         [ 8.6750e-01],\n",
      "         [-4.8375e-02]],\n",
      "\n",
      "        [[ 9.9873e-01],\n",
      "         [-4.8613e-02],\n",
      "         [ 1.3133e-02],\n",
      "         [ 5.0241e-02],\n",
      "         [ 9.7954e-01],\n",
      "         [-1.9487e-01]],\n",
      "\n",
      "        [[ 5.3671e-01],\n",
      "         [ 7.8360e-01],\n",
      "         [-3.1290e-01],\n",
      "         [-8.3018e-01],\n",
      "         [ 5.5670e-01],\n",
      "         [-2.9839e-02]],\n",
      "\n",
      "        [[ 5.6831e-01],\n",
      "         [-7.8313e-01],\n",
      "         [ 2.5244e-01],\n",
      "         [ 8.1789e-01],\n",
      "         [ 5.7119e-01],\n",
      "         [-6.9326e-02]],\n",
      "\n",
      "        [[ 9.5294e-01],\n",
      "         [-1.4606e-01],\n",
      "         [-2.6564e-01],\n",
      "         [ 7.5859e-02],\n",
      "         [ 9.6329e-01],\n",
      "         [-2.5751e-01]],\n",
      "\n",
      "        [[ 9.0807e-01],\n",
      "         [ 2.0764e-01],\n",
      "         [ 3.6371e-01],\n",
      "         [-1.9569e-01],\n",
      "         [ 9.7818e-01],\n",
      "         [-6.9845e-02]],\n",
      "\n",
      "        [[ 9.4058e-01],\n",
      "         [ 2.1949e-01],\n",
      "         [-2.5910e-01],\n",
      "         [-2.2176e-01],\n",
      "         [ 9.7488e-01],\n",
      "         [ 2.0780e-02]],\n",
      "\n",
      "        [[ 9.6967e-01],\n",
      "         [-2.0798e-01],\n",
      "         [ 1.2837e-01],\n",
      "         [ 1.9667e-01],\n",
      "         [ 9.7582e-01],\n",
      "         [ 9.5352e-02]],\n",
      "\n",
      "        [[ 9.1373e-01],\n",
      "         [ 4.0589e-01],\n",
      "         [ 1.8670e-02],\n",
      "         [-4.0118e-01],\n",
      "         [ 9.0850e-01],\n",
      "         [-1.1701e-01]],\n",
      "\n",
      "        [[ 9.1373e-01],\n",
      "         [-4.0589e-01],\n",
      "         [-1.8670e-02],\n",
      "         [ 4.0118e-01],\n",
      "         [ 9.0850e-01],\n",
      "         [-1.1701e-01]],\n",
      "\n",
      "        [[ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00],\n",
      "         [ 0.0000e+00]]]), 'target': 0, 'clip_image': tensor([[[0.8647, 0.8209, 0.8063,  ..., 1.0836, 1.0398, 1.0544],\n",
      "         [0.8209, 0.8063, 0.7917,  ..., 1.0836, 1.0982, 1.0398],\n",
      "         [0.8209, 0.8501, 0.8063,  ..., 1.0690, 1.0544, 1.0544],\n",
      "         ...,\n",
      "         [1.0106, 0.9814, 0.9814,  ..., 1.1420, 1.1712, 1.1274],\n",
      "         [0.9522, 0.9960, 1.0106,  ..., 1.1566, 1.1712, 1.1566],\n",
      "         [0.9668, 0.9960, 1.0106,  ..., 1.1420, 1.1420, 1.1712]],\n",
      "\n",
      "        [[0.9793, 0.9343, 0.9193,  ..., 1.2044, 1.1594, 1.1744],\n",
      "         [0.9343, 0.9193, 0.9043,  ..., 1.2044, 1.2194, 1.1594],\n",
      "         [0.9343, 0.9643, 0.9193,  ..., 1.1894, 1.1744, 1.1744],\n",
      "         ...,\n",
      "         [1.1294, 1.0994, 1.0994,  ..., 1.2645, 1.2945, 1.2495],\n",
      "         [1.0694, 1.1144, 1.1294,  ..., 1.2795, 1.2945, 1.2945],\n",
      "         [1.0844, 1.1144, 1.1294,  ..., 1.2645, 1.2645, 1.2945]],\n",
      "\n",
      "        [[1.1078, 1.0652, 1.0510,  ..., 1.3211, 1.2785, 1.2927],\n",
      "         [1.0652, 1.0510, 1.0367,  ..., 1.3211, 1.3354, 1.2785],\n",
      "         [1.0652, 1.0936, 1.0510,  ..., 1.3069, 1.2927, 1.2927],\n",
      "         ...,\n",
      "         [1.2500, 1.2216, 1.2216,  ..., 1.3780, 1.4065, 1.3638],\n",
      "         [1.1932, 1.2358, 1.2500,  ..., 1.3922, 1.4065, 1.4065],\n",
      "         [1.2074, 1.2358, 1.2500,  ..., 1.3780, 1.3780, 1.4065]]]), 'clip_path': array(['../../data/render\\\\ACCAD_Male2General_c3d_A1- Stand_poses_frame50.png'],\n",
      "      dtype='<U68'), 'clip_text': 'stand'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.__getitem__(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f1646-4e39-4147-b007-8b4d1aab65ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motionclip",
   "language": "python",
   "name": "motionclip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
